{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule, seed_everything, LightningDataModule, Callback, Trainer\n",
    "from hydra_zen import builds, make_config, instantiate, zen, just\n",
    "from omegaconf import MISSING, DictConfig, OmegaConf\n",
    "\n",
    "import lightning as pl\n",
    "import torch\n",
    "from torch_compose.module import ModuleGraph, DirectedModule\n",
    "\n",
    "import wandb\n",
    "\n",
    "from typing import Any, Callable, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenericLitModule(LightningModule):\n",
    "    \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer_partial,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # it also ensures init params will be stored in ckpt\n",
    "        #\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.optimizer_partial = optimizer_partial\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        y_hat = self.forward(batch[0])\n",
    "        loss = self.loss_fn(y_hat, batch[1])\n",
    "        self.log(\"train/loss\", loss, on_epoch=True)\n",
    "        return loss_dict\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        y_hat = self.forward(batch[0])\n",
    "        loss = self.loss_fn(y_hat, batch[1])\n",
    "        self.log_dict({\"valid/loss\": loss, \"valid/prediction\": y_hat, \"valid/targets\": targets})\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n",
    "\n",
    "        See examples here:\n",
    "            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n",
    "        \"\"\"\n",
    "        return self.optimizer_partial(\n",
    "            params=self.parameters(),\n",
    "        )\n",
    "        \n",
    "class TorchComposeLitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        module_graph: ModuleGraph,\n",
    "        optimizer_partial: Callable,\n",
    "        loss_key: str = \"combined_loss\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module_graph = module_graph\n",
    "        self.loss_key = loss_key\n",
    "        self.optimizer_partial = optimizer_partial\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.module_graph.forward(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = self.forward(batch)\n",
    "        loss = batch[self.loss_key]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = self.forward(batch)\n",
    "        loss = batch[self.loss_key]\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optimizer_partial(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictOutput(DirectedModule):\n",
    "    def forward(self, x):\n",
    "        xx = x + 1\n",
    "        return {'x': xx}\n",
    "    \n",
    "    \n",
    "class TupleOutput(DirectedModule):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_squared = x**2\n",
    "        return x, x_squared\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.data_list1 = torch.randn(100, 3, 64, 64)  # e.g. 100 64x64 RGB images\n",
    "        self.data_list2 = torch.randn(100, 10)  # e.g. 100 vectors of length 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item1 = self.data_list1[idx]\n",
    "        item2 = self.data_list2[idx]\n",
    "\n",
    "        return item1, item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001, betas=[0.9, 0.999], eps=1e-08, weight_decay=0, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instantiate(Builds_Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Builds_LitModule = builds(\n",
    "    TorchComposeLitModule,\n",
    "    module_graph=builds(ModuleGraph,\n",
    "        modules={\n",
    "            'm0': builds(DictOutput,input_keys=['x0'], output_keys = {'x': 'x1'}),\n",
    "            'm1': builds(TupleOutput, input_keys=['x1'], output_keys = ['x2', 'x3']),\n",
    "            }),\n",
    "    optimizer_partial=builds(torch.optim.Adam, zen_partial=True, populate_full_signature=True),\n",
    "    populate_full_signature=True,\n",
    "    hydra_recursive=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Builds_LitDataModule = builds(\n",
    "    LitDataModule,\n",
    "    dataset=builds(DummyDataset, populate_full_signature=True,),\n",
    "    populate_full_signature=True,\n",
    ")\n",
    "\n",
    "Builds_Trainer = builds(Trainer, populate_full_signature=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "builds_wandb_run = builds(wandb.init, zen_partial=True, populate_full_signature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_seed(cfg):\n",
    "    seed_everything(cfg.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_run(config):\n",
    "    run = instantiate(config.run)\n",
    "    with run(config=OmegaConf.to_container(OmegaConf.structured(config))) as run:\n",
    "        return train(\n",
    "            model=config.model, \n",
    "            datamodule=config.datamodule, \n",
    "            trainer=config.trainer,\n",
    "            optim=config.optim,\n",
    "            callbacks=config.callbacks,\n",
    "            seed=config.seed,\n",
    "            run=run,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    lit_module, \n",
    "    lit_datamodule, \n",
    "    trainer,\n",
    "    wandb_run,\n",
    "    ):\n",
    "    with wandb_run() as run:\n",
    "        \n",
    "        lit_datamodule.setup()\n",
    "    \n",
    "    # dl = iter(datamodule.train_dataloader())\n",
    "    # batch = next(dl)\n",
    "\n",
    "    # model.forward(batch)\n",
    "    \n",
    "    # trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_wandb_run(cfg):\n",
    "    everything_but = {k:v for k,v in cfg.items() if k not in ['wandb_run']}\n",
    "    cfg.wandb_run.config = everything_but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = make_config(\n",
    "    lit_module=Builds_LitModule,\n",
    "    lit_datamodule = Builds_LitDataModule,\n",
    "    trainer = Builds_Trainer,\n",
    "    wandb_run = builds_wandb_run,\n",
    "    random_seed = just(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "zen_train_func = zen(train, pre_call=[launch_wandb_run, pre_seed], exclude=['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1147, in init\n",
      "    run = wi.init()\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 611, in init\n",
      "    run = Run(\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 537, in __init__\n",
      "    self._init(\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 653, in _init\n",
      "    self._config._update(config, ignore_locked=True)\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 177, in _update\n",
      "    sanitized = self._sanitize_dict(\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 237, in _sanitize_dict\n",
      "    k, v = self._sanitize(k, v, allow_val_change)\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 255, in _sanitize\n",
      "    val = json_friendly_val(val)\n",
      "  File \"/home/naka/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/util.py\", line 653, in json_friendly_val\n",
      "    converted = asdict(val)\n",
      "  File \"/usr/lib/python3.10/dataclasses.py\", line 1239, in asdict\n",
      "    return _asdict_inner(obj, dict_factory)\n",
      "  File \"/usr/lib/python3.10/dataclasses.py\", line 1246, in _asdict_inner\n",
      "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
      "  File \"/usr/lib/python3.10/dataclasses.py\", line 1276, in _asdict_inner\n",
      "    return type(obj)((_asdict_inner(k, dict_factory),\n",
      "TypeError: first argument must be callable or None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function init at 0x7ff3581f7a30>, job_type=None, dir=None, config={'lit_module': TorchComposeLitModule(\n",
      "  (module_graph): ModuleGraph(\n",
      "    (m0): DictOutput()\n",
      "    (m1): TupleOutput()\n",
      "  )\n",
      "), 'lit_datamodule': <__main__.LitDataModule object at 0x7ff3551afee0>, 'trainer': <lightning.pytorch.trainer.trainer.Trainer object at 0x7ff3551ac6a0>, 'random_seed': 1}, project=None, entity=None, reinit=None, tags=None, group=None, name=None, notes=None, magic=None, config_exclude_keys=None, config_include_keys=None, anonymous=None, mode=None, allow_val_change=None, resume=None, force=None, tensorboard=None, sync_tensorboard=None, monitor_gym=None, save_code=None, id=None, settings=None)\n",
      "Problem at: /tmp/ipykernel_86365/877047569.py 8 train\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "An unexpected error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1147\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1148\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:611\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39m# Make sure we are logged in\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[39m# wandb_login._login(_backend=backend, _settings=self.settings)\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \n\u001b[1;32m    610\u001b[0m \u001b[39m# resuming needs access to the server, check server_status()?\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m run \u001b[39m=\u001b[39m Run(\n\u001b[1;32m    612\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    613\u001b[0m     settings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings,\n\u001b[1;32m    614\u001b[0m     sweep_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msweep_config,\n\u001b[1;32m    615\u001b[0m     launch_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlaunch_config,\n\u001b[1;32m    616\u001b[0m )\n\u001b[1;32m    618\u001b[0m \u001b[39m# Populate initial telemetry\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:537\u001b[0m, in \u001b[0;36mRun.__init__\u001b[0;34m(self, settings, config, sweep_config, launch_config)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_pid \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetpid()\n\u001b[0;32m--> 537\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\n\u001b[1;32m    538\u001b[0m     settings\u001b[39m=\u001b[39;49msettings,\n\u001b[1;32m    539\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    540\u001b[0m     sweep_config\u001b[39m=\u001b[39;49msweep_config,\n\u001b[1;32m    541\u001b[0m     launch_config\u001b[39m=\u001b[39;49mlaunch_config,\n\u001b[1;32m    542\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:653\u001b[0m, in \u001b[0;36mRun._init\u001b[0;34m(self, settings, config, sweep_config, launch_config)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mupdate_locked(\n\u001b[1;32m    650\u001b[0m         launch_config, user\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlaunch\u001b[39m\u001b[39m\"\u001b[39m, _allow_val_change\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     )\n\u001b[0;32m--> 653\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config\u001b[39m.\u001b[39;49m_update(config, ignore_locked\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    655\u001b[0m \u001b[39m# interface pid and port configured when backend is configured (See _hack_set_run)\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39m# TODO: using pid isn't the best for windows as pid reuse can happen more often than unix\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_config.py:177\u001b[0m, in \u001b[0;36mConfig._update\u001b[0;34m(self, d, allow_val_change, ignore_locked)\u001b[0m\n\u001b[1;32m    176\u001b[0m         locked_keys\u001b[39m.\u001b[39madd(key)\n\u001b[0;32m--> 177\u001b[0m sanitized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_dict(\n\u001b[1;32m    178\u001b[0m     parsed_dict, allow_val_change, ignore_keys\u001b[39m=\u001b[39;49mlocked_keys\n\u001b[1;32m    179\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mupdate(sanitized)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_config.py:237\u001b[0m, in \u001b[0;36mConfig._sanitize_dict\u001b[0;34m(self, config_dict, allow_val_change, ignore_keys)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize(k, v, allow_val_change)\n\u001b[1;32m    238\u001b[0m sanitized[k] \u001b[39m=\u001b[39m v\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_config.py:255\u001b[0m, in \u001b[0;36mConfig._sanitize\u001b[0;34m(self, key, val, allow_val_change)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(val, wandb\u001b[39m.\u001b[39mArtifact):\n\u001b[0;32m--> 255\u001b[0m     val \u001b[39m=\u001b[39m json_friendly_val(val)\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_val_change:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/util.py:653\u001b[0m, in \u001b[0;36mjson_friendly_val\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[39mif\u001b[39;00m is_dataclass(val) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(val, \u001b[39mtype\u001b[39m):\n\u001b[0;32m--> 653\u001b[0m     converted \u001b[39m=\u001b[39m asdict(val)\n\u001b[1;32m    654\u001b[0m     \u001b[39mreturn\u001b[39;00m converted\n",
      "File \u001b[0;32m/usr/lib/python3.10/dataclasses.py:1239\u001b[0m, in \u001b[0;36masdict\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39masdict() should be called on dataclass instances\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1239\u001b[0m \u001b[39mreturn\u001b[39;00m _asdict_inner(obj, dict_factory)\n",
      "File \u001b[0;32m/usr/lib/python3.10/dataclasses.py:1246\u001b[0m, in \u001b[0;36m_asdict_inner\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields(obj):\n\u001b[0;32m-> 1246\u001b[0m     value \u001b[39m=\u001b[39m _asdict_inner(\u001b[39mgetattr\u001b[39;49m(obj, f\u001b[39m.\u001b[39;49mname), dict_factory)\n\u001b[1;32m   1247\u001b[0m     result\u001b[39m.\u001b[39mappend((f\u001b[39m.\u001b[39mname, value))\n",
      "File \u001b[0;32m/usr/lib/python3.10/dataclasses.py:1276\u001b[0m, in \u001b[0;36m_asdict_inner\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(obj)((_asdict_inner(k, dict_factory),\n\u001b[1;32m   1277\u001b[0m                       _asdict_inner(v, dict_factory))\n\u001b[1;32m   1278\u001b[0m                      \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m obj\u001b[39m.\u001b[39;49mitems())\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: first argument must be callable or None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m zen_train_func(config)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/hydra_zen/wrapper/_implementations.py:380\u001b[0m, in \u001b[0;36mZen.__call__\u001b[0;34m(self, _Zen__cfg)\u001b[0m\n\u001b[1;32m    372\u001b[0m     names \u001b[39m=\u001b[39m (\n\u001b[1;32m    373\u001b[0m         name\n\u001b[1;32m    374\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m cfg\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(name, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    379\u001b[0m     cfg_kwargs\u001b[39m.\u001b[39mupdate({name: cfg[name] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m names})\n\u001b[0;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\n\u001b[1;32m    381\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstantiate(x) \u001b[39mif\u001b[39;49;00m is_instantiable(x) \u001b[39melse\u001b[39;49;00m x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m args_),\n\u001b[1;32m    382\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\n\u001b[1;32m    383\u001b[0m         name: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstantiate(val) \u001b[39mif\u001b[39;49;00m is_instantiable(val) \u001b[39melse\u001b[39;49;00m val\n\u001b[1;32m    384\u001b[0m         \u001b[39mfor\u001b[39;49;00m name, val \u001b[39min\u001b[39;49;00m cfg_kwargs\u001b[39m.\u001b[39;49mitems()\n\u001b[1;32m    385\u001b[0m     },\n\u001b[1;32m    386\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kwargs,\n\u001b[1;32m    387\u001b[0m )\n",
      "Cell \u001b[0;32mIn[180], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lit_module, lit_datamodule, trainer, wandb_run)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m      2\u001b[0m     lit_module, \n\u001b[1;32m      3\u001b[0m     lit_datamodule, \n\u001b[1;32m      4\u001b[0m     trainer,\n\u001b[1;32m      5\u001b[0m     wandb_run,\n\u001b[1;32m      6\u001b[0m     ):\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(wandb_run)\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mwith\u001b[39;00m wandb_run() \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m     10\u001b[0m         lit_datamodule\u001b[39m.\u001b[39msetup()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/torch-compose-LagqJUxr-py3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1185\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             wandb\u001b[39m.\u001b[39mtermerror(\u001b[39m\"\u001b[39m\u001b[39mAbnormal program exit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1184\u001b[0m             os\u001b[39m.\u001b[39m_exit(\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1185\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mAn unexpected error occurred\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror_seen\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[0;31mError\u001b[0m: An unexpected error occurred"
     ]
    }
   ],
   "source": [
    "zen_train_func(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-compose-LagqJUxr-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
